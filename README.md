# Stepwise DPO Fine-Tuning with TinyLlama

This project implements a Stepwise Direct Preference Optimization (DPO) fine-tuning pipeline using the TinyLlama-1.1B-Chat model. The pipeline scores intermediate reasoning steps, trains a preference model on them, and evaluates performance through reward model comparison.

---

## ğŸ“¦ Dataset & Model

- **Base Model**: [`TinyLlama-1.1B-Chat-v1.0`](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)
- **Data Format**: JSONL with:
  - `prompt`: reasoning task
  - `chosen`: correct multi-step response
  - `rejected`: incorrect multi-step response
  - `chosen_scores`, `rejected_scores`: list of stepwise scores (1 or 0)

- **Fine-Tuning Method**: Stepwise-DPO  
  We break down the full responses into reasoning steps and apply DPO loss on mean stepwise scores:
  
  \[
  \text{loss} = -\log(\sigma(R_\text{chosen} - R_\text{rejected}))
  \]

---

## ğŸ“ˆ Reward Score Evaluation

Using a 4-bit TinyLlama reward model (`ctransformers`), we scored both the base and fine-tuned models on 15 examples.

| Metric               | Value       |
|----------------------|-------------|
| Base Avg Score       | 0.0000      |
| Fine-Tuned Avg Score | 0.0667      |
| âœ… Improvement       | +0.0667     |
| Evaluation Size      | 15 examples |

---

## âš ï¸ Limitations & Next Steps

- **Limited Data**: Only 15 examples were used. Results may not generalize.
- **No Human Raters**: Stepwise scores are heuristic, not human-labeled.
- **Eval Speed**: Inference using CPU 4-bit models is slow for large-scale eval.
- **Model Type**: GGUF 4-bit model limits compatibility with standard HF APIs.

### âœ… Future Improvements:
- Scale to 1k+ examples
- Use human annotators for stepwise labels
- Evaluate with larger reward models or LLM-as-a-judge

---

## ğŸš€ How to Run

### ğŸ“¦ Setup
```bash

# 1. Clone and setup

git clone https://github.com/FRAGGERR/Reward-Model-DPO/tree/main
cd futureagi-ml-intern


# 2. Create virtual env
conda create -n dpo-env python=3.11
conda activate dpo-env

# 3. Install dependencies
pip install -r requirements.txt

ğŸ‹ï¸â€â™‚ï¸ Training

python train.py


ğŸ” Evaluation

cd evaluation
python eval_reward_scores.py

ğŸ§  Inference

python inference.py

```
--- 

## ğŸ“ Project Structure

futureagi-ml-intern/
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ inference.py
â”‚
â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ stepwise_dpo_trainer.py
â”‚
â”œâ”€â”€ reward_model/
â”‚   â”œâ”€â”€ evaluate_steps.py
â”‚   â”œâ”€â”€ phi2_reward_model.py
â”‚   â”œâ”€â”€ test_reward.py
â”‚   â””â”€â”€ tinyllama_reward_model.py
â”œâ”€â”€ saved_model/
â”‚   â””â”€â”€ stepwise_dpo_tinyllama/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ tinyllama/
â”‚       â””â”€â”€ tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_dataset.jsonl
â”‚   â””â”€â”€ rewarded.jsonl
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ build_synthetic_dataset.py
â”‚   â”œâ”€â”€ load_dataset.py
â”‚   â”œâ”€â”€ load_rewarded_dataset.py
â”‚   â””â”€â”€ test_dataset.py 
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval_reward_scores.py
â”‚   â””â”€â”€ reward_score_comparison.json
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_log.txt
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LLM_USAGE.md

---

## ğŸ™Œ Acknowledgments
#### This project was implemented as part of the FutureAGI ML Intern Assignment, combining modern fine-tuning (DPO) with step-level interpretability.
